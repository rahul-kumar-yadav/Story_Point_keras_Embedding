{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Story_point_estimation_Keras_Embedding_layer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPmSEfvco9OAvT9P8TUY1Yi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahul-kumar-yadav/Story_Point_keras_Embedding/blob/main_/Story_point_estimation_Keras_Embedding_layer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0L2SK0pQTmoe",
        "outputId": "b5a31cbd-3d80-42d0-a857-76eefa5023f8"
      },
      "source": [
        "import pandas\r\n",
        "from numpy import array\r\n",
        "from keras.preprocessing.text import one_hot\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import Flatten\r\n",
        "from keras.layers.embeddings import Embedding\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\r\n",
        "from keras.utils import np_utils\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "from sklearn.model_selection import KFold\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.compose import make_column_transformer\r\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\r\n",
        "# load dataset\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import re\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('wordnet')\r\n",
        "from nltk.corpus import stopwords\r\n",
        "import string\r\n",
        "# from word2vec_keras import Word2VecKeras\r\n",
        "# Word2vec\r\n",
        "import gensim\r\n",
        "from tensorflow.keras import layers\r\n",
        "\r\n",
        "\r\n",
        "# Scikit learn\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\r\n",
        "\r\n",
        "# Keras\r\n",
        "from tensorflow import keras\r\n",
        "from keras.models import Sequential, load_model\r\n",
        "from keras.layers import Dense, Dropout, Embedding, LSTM\r\n",
        "from keras import utils\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\r\n",
        "\r\n",
        "# Utility\r\n",
        "import numpy as np\r\n",
        "import tarfile\r\n",
        "import pickle\r\n",
        "import tempfile\r\n",
        "import os\r\n",
        "import time\r\n",
        "import logging\r\n",
        "import multiprocessing\r\n",
        "\r\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "df = pd.read_csv(\"jirasoftware.csv\", usecols=['issuekey', 'title', 'description', 'storypoint'])\r\n",
        "# df.isnull().sum()\r\n",
        "\t\r\n",
        "df = df.dropna(how='any')\r\n",
        "\r\n",
        "df.head()\r\n",
        "\r\n",
        "df.loc[df.storypoint <= 3, 'storypoint'] = 0 #small\r\n",
        "df.loc[(df.storypoint > 3) & (df.storypoint <= 8), 'storypoint'] = 1 #medium\r\n",
        "df.loc[df.storypoint > 8, 'storypoint'] = 2 #big\r\n",
        "htmltokens = ['{html}','<div>','<pre>','<p>', '</div>','</pre>','</p>']\r\n",
        "\r\n",
        "def removeHtml(text):\r\n",
        "  for w in htmltokens:\r\n",
        "        text = text.replace(w, '')\r\n",
        "  return text\r\n",
        "df['description'] = df[\"description\"].apply(lambda x: re.sub(r'http\\S+','',x))\r\n",
        "def cleanData(text):\r\n",
        "  result = ''\r\n",
        "  str1=\"\"\r\n",
        "  list1=[]\r\n",
        "  for c in text:\r\n",
        "    if c not in string.punctuation:\r\n",
        "      list1.append(c)\r\n",
        "      str1 = \"\".join(list1)\r\n",
        "    else:\r\n",
        "      list1.append(\" \")\r\n",
        "      str1 = \"\".join(list1)\r\n",
        "  \r\n",
        "  char_string = ' '.join([w for w in str1.split() if len(w) > 2])\r\n",
        "\r\n",
        "\r\n",
        "  text_words = char_string.split()\r\n",
        "  resultwords=[word for word in text_words if word not in stopwords.words('english')]\r\n",
        "\r\n",
        "  if len(resultwords) > 0:\r\n",
        "    result=' '.join(resultwords)\r\n",
        "  else:\r\n",
        "    print(\"empty text\")\r\n",
        "  return result\r\n",
        "\r\n",
        "\r\n",
        "df['label_title_desc'] = df['title'].str.lower() + ' - ' + df['description'].str.lower()\r\n",
        "df['label_title_desc'] = df['label_title_desc'].apply(lambda x:removeHtml(str(x)))\r\n",
        "df['title_desc'] = df['label_title_desc'].apply(lambda x: cleanData(str(x)))\r\n",
        "\r\n",
        "X_train_split, X_test, y_train_split, y_test = train_test_split(df['title_desc'], df['storypoint'], train_size=0.8, random_state=42)\r\n",
        "from collections import Counter\r\n",
        "\r\n",
        "def SimpleOverSample(_xtrain, _ytrain):\r\n",
        "    xtrain = list(_xtrain)\r\n",
        "    ytrain = list(_ytrain)\r\n",
        "\r\n",
        "    samples_counter = Counter(ytrain)\r\n",
        "    max_samples = sorted(samples_counter.values(), reverse=True)[0]\r\n",
        "    for sc in samples_counter:\r\n",
        "        init_samples = samples_counter[sc]\r\n",
        "        samples_to_add = max_samples - init_samples\r\n",
        "        if samples_to_add > 0:\r\n",
        "            #collect indices to oversample for the current class\r\n",
        "            index = list()\r\n",
        "            for i in range(len(ytrain)):\r\n",
        "                if(ytrain[i] == sc):\r\n",
        "                    index.append(i)\r\n",
        "            #select samples to copy for the current class    \r\n",
        "            copy_from = [xtrain[i] for i in index]\r\n",
        "            index_copy = 0\r\n",
        "            for i in range(samples_to_add):\r\n",
        "                xtrain.append(copy_from[index_copy % len(copy_from)])\r\n",
        "                ytrain.append(sc)\r\n",
        "                index_copy += 1\r\n",
        "    return xtrain, ytrain\r\n",
        "\r\n",
        "X_train,y_train = SimpleOverSample(X_train_split, y_train_split)\r\n",
        "\r\n",
        "vocab_size = 20000\r\n",
        "label_encoder = LabelEncoder()\r\n",
        "y_train = label_encoder.fit_transform(y_train)\r\n",
        "num_classes = len(label_encoder.classes_)\r\n",
        "y_train = utils.to_categorical(y_train, num_classes)\r\n",
        "\r\n",
        "y_test = label_encoder.fit_transform(y_test)\r\n",
        "num_classes = len(label_encoder.classes_)\r\n",
        "y_test = utils.to_categorical(y_test, num_classes)\r\n",
        "\r\n",
        "X_train = [one_hot(d, vocab_size,filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',lower=True, split=' ') for d in X_train]\r\n",
        "X_test = [one_hot(d, vocab_size,filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',lower=True, split=' ') for d in X_test]\r\n",
        "\r\n",
        "max_length = 100\r\n",
        "X_train = pad_sequences(X_train, maxlen=max_length, padding='pre')\r\n",
        "X_test = pad_sequences(X_test, maxlen=max_length, padding='pre')\r\n",
        "\r\n",
        "model = Sequential()\r\n",
        "model.add(Embedding(vocab_size, 32, input_length=max_length))\r\n",
        "# model.add(Flatten())\r\n",
        "model.add(LSTM(128, dropout=0.5, recurrent_dropout=0.2))\r\n",
        "model.add(Dense(128, activation='relu'))\r\n",
        "# layers.Dropout(0.3),\r\n",
        "# layers.BatchNormalization(),\r\n",
        "model.add(Dense(64, activation='relu'))\r\n",
        "# layers.Dropout(0.3),\r\n",
        "# layers.BatchNormalization(),\r\n",
        "model.add(Dense(32, activation='relu'))\r\n",
        "# layers.Dropout(0.3),\r\n",
        "# layers.BatchNormalization(),\r\n",
        "model.add(Dense(16, activation='relu'))\r\n",
        "# layers.Dropout(0.3),\r\n",
        "# layers.BatchNormalization(),\r\n",
        "model.add(Dense(num_classes, activation='softmax'))\r\n",
        "early_stopping = EarlyStopping(patience=30, min_delta=0.001, restore_best_weights=True)\r\n",
        "callbacks = [early_stopping]\r\n",
        "\r\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\r\n",
        "\r\n",
        "print(model.summary())\r\n",
        "\r\n",
        "model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=60, callbacks=callbacks, verbose=1)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=1)\r\n",
        "print('Training Accuracy is {}'.format(accuracy*100))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "loss, accuracy = model.evaluate(X_test,y_test)\r\n",
        "print('Testing Accuracy is {} '.format(accuracy*100))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Model: \"sequential_40\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_40 (Embedding)     (None, 100, 32)           640000    \n",
            "_________________________________________________________________\n",
            "lstm_37 (LSTM)               (None, 128)               82432     \n",
            "_________________________________________________________________\n",
            "dense_179 (Dense)            (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_180 (Dense)            (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_181 (Dense)            (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dense_182 (Dense)            (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dense_183 (Dense)            (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 749,859\n",
            "Trainable params: 749,859\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/60\n",
            "11/11 [==============================] - 6s 283ms/step - loss: 1.0985 - acc: 0.2939 - val_loss: 1.0859 - val_acc: 0.5862\n",
            "Epoch 2/60\n",
            "11/11 [==============================] - 3s 245ms/step - loss: 1.0873 - acc: 0.4111 - val_loss: 1.0311 - val_acc: 0.6034\n",
            "Epoch 3/60\n",
            "11/11 [==============================] - 3s 244ms/step - loss: 0.9904 - acc: 0.5868 - val_loss: 0.8831 - val_acc: 0.3276\n",
            "Epoch 4/60\n",
            "11/11 [==============================] - 3s 243ms/step - loss: 0.8077 - acc: 0.6232 - val_loss: 0.8351 - val_acc: 0.3448\n",
            "Epoch 5/60\n",
            "11/11 [==============================] - 3s 244ms/step - loss: 0.6054 - acc: 0.6667 - val_loss: 0.8016 - val_acc: 0.3621\n",
            "Epoch 6/60\n",
            "11/11 [==============================] - 3s 241ms/step - loss: 0.5004 - acc: 0.6693 - val_loss: 0.7063 - val_acc: 0.3793\n",
            "Epoch 7/60\n",
            "11/11 [==============================] - 3s 242ms/step - loss: 0.4650 - acc: 0.6487 - val_loss: 0.6967 - val_acc: 0.3621\n",
            "Epoch 8/60\n",
            "11/11 [==============================] - 3s 245ms/step - loss: 0.4477 - acc: 0.6809 - val_loss: 0.6993 - val_acc: 0.3621\n",
            "Epoch 9/60\n",
            "11/11 [==============================] - 3s 246ms/step - loss: 0.4467 - acc: 0.7129 - val_loss: 0.6718 - val_acc: 0.5345\n",
            "Epoch 10/60\n",
            "11/11 [==============================] - 3s 244ms/step - loss: 0.4382 - acc: 0.7043 - val_loss: 0.6976 - val_acc: 0.4483\n",
            "Epoch 11/60\n",
            "11/11 [==============================] - 3s 247ms/step - loss: 0.4219 - acc: 0.7263 - val_loss: 0.6416 - val_acc: 0.6724\n",
            "Epoch 12/60\n",
            "11/11 [==============================] - 3s 248ms/step - loss: 0.3706 - acc: 0.8120 - val_loss: 0.6238 - val_acc: 0.7414\n",
            "Epoch 13/60\n",
            "11/11 [==============================] - 3s 257ms/step - loss: 0.3381 - acc: 0.8651 - val_loss: 0.5332 - val_acc: 0.7414\n",
            "Epoch 14/60\n",
            "11/11 [==============================] - 3s 247ms/step - loss: 0.2401 - acc: 0.9488 - val_loss: 0.6608 - val_acc: 0.6897\n",
            "Epoch 15/60\n",
            "11/11 [==============================] - 3s 247ms/step - loss: 0.1767 - acc: 0.9632 - val_loss: 0.9070 - val_acc: 0.7414\n",
            "Epoch 16/60\n",
            "11/11 [==============================] - 3s 250ms/step - loss: 0.1378 - acc: 0.9701 - val_loss: 1.1327 - val_acc: 0.6379\n",
            "Epoch 17/60\n",
            "11/11 [==============================] - 3s 252ms/step - loss: 0.1070 - acc: 0.9766 - val_loss: 0.5290 - val_acc: 0.7931\n",
            "Epoch 18/60\n",
            "11/11 [==============================] - 3s 247ms/step - loss: 0.0610 - acc: 0.9820 - val_loss: 0.7106 - val_acc: 0.8103\n",
            "Epoch 19/60\n",
            "11/11 [==============================] - 3s 249ms/step - loss: 0.0103 - acc: 0.9993 - val_loss: 0.7216 - val_acc: 0.8276\n",
            "Epoch 20/60\n",
            "11/11 [==============================] - 3s 245ms/step - loss: 0.0124 - acc: 0.9958 - val_loss: 0.7800 - val_acc: 0.8276\n",
            "Epoch 21/60\n",
            "11/11 [==============================] - 3s 246ms/step - loss: 0.0095 - acc: 0.9962 - val_loss: 0.7180 - val_acc: 0.8276\n",
            "Epoch 22/60\n",
            "11/11 [==============================] - 3s 248ms/step - loss: 0.0032 - acc: 1.0000 - val_loss: 0.8002 - val_acc: 0.8103\n",
            "Epoch 23/60\n",
            "11/11 [==============================] - 3s 245ms/step - loss: 0.0024 - acc: 1.0000 - val_loss: 0.8233 - val_acc: 0.8103\n",
            "Epoch 24/60\n",
            "11/11 [==============================] - 3s 245ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.8622 - val_acc: 0.8103\n",
            "Epoch 25/60\n",
            "11/11 [==============================] - 3s 245ms/step - loss: 6.9514e-04 - acc: 1.0000 - val_loss: 0.9058 - val_acc: 0.8103\n",
            "Epoch 26/60\n",
            "11/11 [==============================] - 3s 252ms/step - loss: 6.4276e-04 - acc: 1.0000 - val_loss: 0.9412 - val_acc: 0.8276\n",
            "Epoch 27/60\n",
            "11/11 [==============================] - 3s 247ms/step - loss: 4.4528e-04 - acc: 1.0000 - val_loss: 0.9648 - val_acc: 0.8276\n",
            "Epoch 28/60\n",
            "11/11 [==============================] - 3s 249ms/step - loss: 4.9531e-04 - acc: 1.0000 - val_loss: 0.9919 - val_acc: 0.8276\n",
            "Epoch 29/60\n",
            "11/11 [==============================] - 3s 248ms/step - loss: 4.4680e-04 - acc: 1.0000 - val_loss: 1.0210 - val_acc: 0.8276\n",
            "Epoch 30/60\n",
            "11/11 [==============================] - 3s 251ms/step - loss: 3.6598e-04 - acc: 1.0000 - val_loss: 1.0470 - val_acc: 0.8276\n",
            "Epoch 31/60\n",
            "11/11 [==============================] - 3s 246ms/step - loss: 3.2789e-04 - acc: 1.0000 - val_loss: 1.0660 - val_acc: 0.8276\n",
            "Epoch 32/60\n",
            "11/11 [==============================] - 3s 246ms/step - loss: 2.7971e-04 - acc: 1.0000 - val_loss: 1.0772 - val_acc: 0.8276\n",
            "Epoch 33/60\n",
            "11/11 [==============================] - 3s 244ms/step - loss: 2.0948e-04 - acc: 1.0000 - val_loss: 1.0944 - val_acc: 0.8276\n",
            "Epoch 34/60\n",
            "11/11 [==============================] - 3s 250ms/step - loss: 4.2905e-04 - acc: 1.0000 - val_loss: 1.0943 - val_acc: 0.8103\n",
            "Epoch 35/60\n",
            "11/11 [==============================] - 3s 247ms/step - loss: 1.4417e-04 - acc: 1.0000 - val_loss: 1.1009 - val_acc: 0.8103\n",
            "Epoch 36/60\n",
            "11/11 [==============================] - 3s 246ms/step - loss: 2.3409e-04 - acc: 1.0000 - val_loss: 1.1121 - val_acc: 0.8103\n",
            "Epoch 37/60\n",
            "11/11 [==============================] - 3s 252ms/step - loss: 1.8789e-04 - acc: 1.0000 - val_loss: 1.1250 - val_acc: 0.8103\n",
            "Epoch 38/60\n",
            "11/11 [==============================] - 3s 248ms/step - loss: 1.1719e-04 - acc: 1.0000 - val_loss: 1.1395 - val_acc: 0.8103\n",
            "Epoch 39/60\n",
            "11/11 [==============================] - 3s 246ms/step - loss: 2.8039e-04 - acc: 1.0000 - val_loss: 1.1234 - val_acc: 0.8276\n",
            "Epoch 40/60\n",
            "11/11 [==============================] - 3s 247ms/step - loss: 1.0256e-04 - acc: 1.0000 - val_loss: 1.1091 - val_acc: 0.8276\n",
            "Epoch 41/60\n",
            "11/11 [==============================] - 3s 255ms/step - loss: 1.9930e-04 - acc: 1.0000 - val_loss: 1.1264 - val_acc: 0.8276\n",
            "Epoch 42/60\n",
            "11/11 [==============================] - 3s 246ms/step - loss: 8.4018e-05 - acc: 1.0000 - val_loss: 1.1463 - val_acc: 0.8276\n",
            "Epoch 43/60\n",
            "11/11 [==============================] - 3s 251ms/step - loss: 6.0361e-05 - acc: 1.0000 - val_loss: 1.1618 - val_acc: 0.8276\n",
            "Epoch 44/60\n",
            "11/11 [==============================] - 3s 247ms/step - loss: 8.1879e-05 - acc: 1.0000 - val_loss: 1.1760 - val_acc: 0.8276\n",
            "Epoch 45/60\n",
            "11/11 [==============================] - 3s 247ms/step - loss: 7.5998e-05 - acc: 1.0000 - val_loss: 1.1888 - val_acc: 0.8276\n",
            "Epoch 46/60\n",
            "11/11 [==============================] - 3s 248ms/step - loss: 8.5227e-05 - acc: 1.0000 - val_loss: 1.1988 - val_acc: 0.8276\n",
            "Epoch 47/60\n",
            "11/11 [==============================] - 3s 247ms/step - loss: 7.8811e-05 - acc: 1.0000 - val_loss: 1.2102 - val_acc: 0.8276\n",
            "11/11 [==============================] - 0s 29ms/step - loss: 0.0538 - acc: 0.9943\n",
            "Training Accuracy is 99.4301974773407\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.5290 - acc: 0.7931\n",
            "Testing Accuracy is 79.31034564971924 \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}